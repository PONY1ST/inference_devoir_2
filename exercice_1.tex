\documentclass[../main.tex]{subfiles}

\begin{document}
\begin{CJK*}{UTF8}{gbsn}

\section*{Exercice 1}

Soit $X_1, X_2, \cdots, X_n$ un échantillon de taille $n$ avec densité 
$f_{\theta}(x) = \frac{1 + \theta x}{2} \chi_{[-1,1]}(x)$, 
où $\theta \in [-1,1]$ est inconnu. Écrire $Y_i = \chi_{[0,1]}(X_i)$
pour tout $i \in \curly{ 1, 2, \cdots, n}$.
Estimer $\theta$ par la méthode des moments utilisant $\{X_i\}_{i=1}^n$ et 
calculer l'écart quadratique moyenne. 
Proposez un autre estimateur de $\theta$ qui améliore cet écart quadratique moyenne.
Répétez les mêmes tâches utilisant $\{Y_i\}_{i=1}^n$ et pas $\{X_i\}_{i=1}^n$.

\paragraph{Solution}

L'espérance de $X_1$, $E[X_1]$, s'obtient en intégrant le produit de $X_1$ par la densité de probabilité sur l'intervalle de définition de $X_1$, ce qui nous donne :
\begin{equation*}
E[X_1] = \int_{-1}^{1} x_1 \cdot \frac{1 + \theta x_1}{2} \, dx_1
\end{equation*}

Cette intégration aboutit à :
\begin{equation*}
E[X_1] = \frac{\theta}{3} 
\end{equation*}

\begin{equation*}
E[X_1^2] = \frac{1}{3} 
\end{equation*}

En utilisant la méthode des moments, on égalise le moment théorique, 
ici l'espérance calculée $E[X_1]$, 
avec le moment empirique, la moyenne d'échantillon $\bar{X}$, ce qui nous donne l'équation :
\begin{equation*}
\bar{X} = \frac{\theta}{3}
\end{equation*}
En résolvant cette équation pour $\theta$, nous obtenons l'estimateur de $\theta$ par la méthode des moments,$\hat{\theta_x}$ :
\begin{equation*}
\hat{\theta_x} = 3\bar{X}
\end{equation*}

\begin{equation*}
\hat{\theta_x} = 3\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)
\end{equation*}

L'écart quadratique moyen  d'un estimateur $\hat{\theta_x}$ est défini par :
\begin{equation*}
EQM(\hat{\theta_x}) = E[(\hat{\theta_x} - \theta)^2] = E\left[9\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right)^2 - 6\theta\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) + \theta^2\right]
\end{equation*}

On procède maintenant à la substitution de $E[X_1]$ et $E[X_1^2]$ dans notre expression précédente pour simplifier le calcul.

\begin{equation*}
EQM(\hat{\theta_x}) = \frac{3 - \theta^2}{n}
\end{equation*}


La contribution attendue de $X_1$ sous cette condition, ou l'espérance de $Y_1$, est calculée comme suit:
\begin{equation*}
E[Y_1] = \int_{0}^{1} \frac{1 + \theta x_1}{2} dx_1 = \frac{1}{2} + \frac{\theta}{4}
\end{equation*}

Comme $Y_1$ est une fonction indicatrice dont les valeurs peuvent uniquement être $0$ ou $1$, la valeur de $Y_1^2$ est en réalité identique à celle de $Y_1$. 
Cela est dû au fait que $1^2 = 1$ et $0^2 = 0$. Par conséquent, nous avons $E[Y_1^2] = E[Y_1]$.

Par la méthode des moments, l'alignement de l'espérance théorique $E[Y_1]$ avec la moyenne empirique des observations de $Y_1$ donne :
\begin{equation*}
E[Y_1] = \frac{1}{2} + \frac{\theta}{4} = \bar{Y} = \frac{1}{n}\sum_{i=1}^{n} Y_i.
\end{equation*}

Résoudre cette équation pour $\theta$, on introduis l'estimateur $\hat{\theta_y}$, qui est défini et obtenu par :
\begin{equation*}
\hat{\theta_y} = 4\left(\frac{1}{n}\sum_{i=1}^{n} Y_i\right) - 2,
\end{equation*}
offrant une estimation directe de $\theta$ basée sur la moyenne d'échantillon des $Y_i$.

L'écart quadratique moyen  d'un estimateur $\hat{\theta_y}$ est défini par :
\begin{equation*}
EQM(\hat{\theta_y}) = E[(\hat{\theta_y} - \theta)^2] = E\left[16\left(\frac{1}{n}\sum_{i=1}^{n} Y_i\right)^2 - 8\left(\frac{1}{n}\sum_{i=1}^{n} Y_i\right)(2 + \theta) + (2 + \theta)^2\right]
\end{equation*}

On procède maintenant à la substitution de $E[Y_1]$ et $E[Y_1^2]$ dans notre expression précédente pour simplifier
le calcul：
\begin{equation*}
EQM(\hat{\theta_y}) = \frac{4 - \theta^2}{n}
\end{equation*}

Considérant l'estimateur \(\hat{\theta}_x = 3\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)\), avec \(X_i \in [-1, 1]\), 
et \( \theta \in [-1, 1] \), 
l'espace image de $\hat{\theta}_x$ est $[-3, 3]$, 
couvre et dépasse l'intervalle de $\theta$. 
Cela indique que $\hat{\theta}_x$ peut générer des estimations 
au-delà des valeurs réelles possibles de $\theta$.

Définissons la valeur ajustée $\hat{\theta}_{x}^{adj} $ comme suit :
\begin{equation*}
\hat{\theta}_{x}^{adj} = \hat{\theta}_x \cdot I(|\hat{\theta}_x| \leq 1) + 1 \cdot I(\hat{\theta}_x > 1) - 1 \cdot I(\hat{\theta}_x < -1)
\end{equation*}

On découvre que, grâce à l'ajustement post-traitement, 
l'erreur quadratique moyenne de l'estimateur ajusté est réduite par rapport à l'estimateur initial. 
Cela peut être exprimé mathématiquement comme suit :
\begin{equation*}
E\left[(\hat{\theta}_{x}^{adj} - \theta)^2\right] \leq E\left[(\hat{\theta}_x - \theta)^2\right],
\end{equation*}
ce qui indique que cette méthode réussit à améliorer les écarts quadratiques moyens.


\end{CJK*}
\end{document}
